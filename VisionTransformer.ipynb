{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DUNGTK2004/deep-models-from-scratch/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dependencies"
      ],
      "metadata": {
        "id": "tL1rsB_WPUWQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "v53V1YaLDnBc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "C1IQcaa5PaE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    # T.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "    # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "IKbgiCzyGP2w"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "dUJAmbjdExQa"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "-JHqYTynEzzb"
      },
      "outputs": [],
      "source": [
        "mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irryk7vHE7EP"
      },
      "source": [
        "### Model pretrained ViT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "XpE0iPeXHbMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWrw2PXNFCJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235d3ee9-0877-40d5-c432-d4a73798ee15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:06<00:00, 56.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[256]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "# for parameter in pretrained_vit.parameters():\n",
        "#   parameter.requires_grad = False\n",
        "\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_vit\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdXv7TfNJD8D",
        "outputId": "a3079f86-40a7-4b8f-a5c9-f90995ad19c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86567656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT from scratch"
      ],
      "metadata": {
        "id": "NFr3Amr_9CGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "  B, C, H, W = x.shape\n",
        "  x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size) # B, C, H', P, W', P\n",
        "  x = x.permute(0, 2, 4, 1, 3, 5) # B, H', W', C, P, P\n",
        "  x = x.flatten(1, 2)             # B, H' * W', C, P, P\n",
        "  if flatten_channels:\n",
        "    x = x.flatten(2, 4)           # B, H' * W', C * P * P\n",
        "  return x"
      ],
      "metadata": {
        "id": "6xe5VdUoGhA2"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the patches of image"
      ],
      "metadata": {
        "id": "qg7vs1dGJCw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = mnist_trainset[0][0].unsqueeze(0)\n",
        "_, C, H, W = image.shape\n",
        "print(image.shape)\n",
        "img_patches = img_to_patch(image, patch_size=7, flatten_channels=False)\n",
        "img_grid = torchvision.utils.make_grid(img_patches[0], nrow=H//7, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "plt.imshow(img_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "wxF2pJAZJwzy",
        "outputId": "9144113d-7bc6-4b08-f140-e68a791b54c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28])\n",
            "1 1 28 28 7\n",
            "torch.Size([1, 16, 1, 7, 7])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7afc2dc911d0>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test multihead attention from scratch but not efficiently"
      ],
      "metadata": {
        "id": "sMxfYAYJZMGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, d_model, head_size, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "    self.query = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.key = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.value = nn.Linear(d_model, head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    Q = self.query(x)\n",
        "    K = self.key(x)\n",
        "    V = self.value(x)\n",
        "\n",
        "    attention = Q @ K.transpose(-2, -1)\n",
        "    attention = attention / (self.head_size ** 0.5)\n",
        "    attention = torch.softmax(attention, dim=-1)\n",
        "    attention= self.dropout(attention)\n",
        "    attention = attention @ V\n",
        "\n",
        "    # attention = F.scaled_dot_product_attention(Q, K, V, dropout_p=0.1)\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "s6RNNNaYR41p"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.head_size = d_model // n_heads\n",
        "\n",
        "    self.W_o = nn.Linear(d_model, d_model)\n",
        "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
        "    self.layer_norm = nn.LayerNorm(d_model)\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    out = self.W_o(out)\n",
        "    return self.layer_norm(out)\n"
      ],
      "metadata": {
        "id": "5qRGbzGqTekj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim, embed_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    input = self.layer_norm1(x)\n",
        "    x = x + self.mha(input, input, input)[0]\n",
        "    x = x + self.mlp(self.layer_norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "fi0gOp6u9E_z"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, num_channel, patch_size, embed_dim, num_patch,\n",
        "               hidden_dim, num_heads, num_layers, num_classes, dropout=0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.input_layer = nn.Linear(num_channel * (patch_size ** 2), embed_dim) # because output of patch image layer is num_channels * (P ** 2)\n",
        "    self.cls_token = nn.Parameter(torch.rand(1, 1, embed_dim))\n",
        "    self.position_embedding = nn.Parameter(torch.rand(1, 1 + num_patch, embed_dim))\n",
        "\n",
        "    self.transformer_encoder = nn.Sequential(\n",
        "        *(AttentionBlock(embed_dim, hidden_dim, num_heads, dropout) for _ in range(num_layers))\n",
        "    )\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.LayerNorm(embed_dim),\n",
        "        nn.Linear(embed_dim, num_classes)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = img_to_patch(x, self.patch_size, flatten_channels=True) # batch, num_patch, (c * patch_size ** 2)\n",
        "    B, N, _ = x.shape\n",
        "    x = self.input_layer(x)\n",
        "\n",
        "    cls_token = self.cls_token.repeat(B, 1, 1)\n",
        "    x = torch.cat([cls_token, x], dim=1)\n",
        "    x = x + self.position_embedding[:, : N + 1]\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = x.transpose(1, 0) # -> num_patch+1, batch_size, embed để phù hợp với output transformer\n",
        "    x = self.transformer_encoder(x)\n",
        "    cls_token = x[0]\n",
        "    x = self.mlp(cls_token)\n",
        "    x = self.dropout(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "SIXPK8TSWQAg"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter\n",
        "num_channel = 1\n",
        "patch_size = 7\n",
        "embed_dim = 256\n",
        "num_patch = 16\n",
        "hidden_dim = embed_dim * 3\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "num_classes = 10\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "2-Qe7JNhHsxn"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(num_channel=num_channel, patch_size=patch_size,\n",
        "                          embed_dim=embed_dim, num_patch=num_patch,\n",
        "                          hidden_dim=hidden_dim, num_heads=num_heads,\n",
        "                          num_layers=num_layers, num_classes=num_classes,\n",
        "                          dropout=dropout)"
      ],
      "metadata": {
        "id": "gjZX9c4gGpnz"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaCMyDdlP3QJ",
        "outputId": "cbe08adb-9f58-451d-b122-496ef09b0a98"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3971082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwfQgjo3J28P",
        "outputId": "b53ccb8b-6fc2-478d-c3cc-42dce7b438cf"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (input_layer): Linear(in_features=49, out_features=256, bias=True)\n",
              "  (transformer_encoder): Sequential(\n",
              "    (0): AttentionBlock(\n",
              "      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): AttentionBlock(\n",
              "      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): AttentionBlock(\n",
              "      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): AttentionBlock(\n",
              "      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): AttentionBlock(\n",
              "      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): AttentionBlock(\n",
              "      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mha): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Dropout(p=0.2, inplace=False)\n",
              "        (3): Linear(in_features=768, out_features=256, bias=True)\n",
              "        (4): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp): Sequential(\n",
              "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/test"
      ],
      "metadata": {
        "id": "dDLY5IQqPfyN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_1Tce2akE_2H"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(data_iter, net):\n",
        "  acc_sum, n = 0, 0\n",
        "  for (imgs, labels) in data_iter:\n",
        "    # send data to the GPU if cuda is available\n",
        "    if torch.cuda.is_available():\n",
        "      imgs = imgs.cuda()\n",
        "      labels = labels.cuda()\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "      labels = labels.long()\n",
        "      acc_sum += torch.sum((torch.argmax(net(imgs), dim=1) == labels)).float()\n",
        "      n += labels.shape[0]\n",
        "\n",
        "  return acc_sum.item()/n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "9C7EzrboE8jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a044d8b1-6ae6-48c2-9db4-2a0aa0f09399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:26, 35.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 0.0207, train acc 0.536, test acc 0.828, time 28.8 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:23, 39.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, loss 0.0130, train acc 0.713, test acc 0.875, time 26.3 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:24, 39.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, loss 0.0101, train acc 0.769, test acc 0.911, time 26.1 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:23, 39.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, loss 0.0094, train acc 0.780, test acc 0.899, time 25.9 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:24, 38.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, loss 0.0087, train acc 0.796, test acc 0.922, time 26.1 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:24, 38.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, loss 0.0094, train acc 0.782, test acc 0.909, time 26.7 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:23, 39.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, loss 0.0084, train acc 0.799, test acc 0.926, time 26.4 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:24, 38.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8, loss 0.0086, train acc 0.795, test acc 0.917, time 26.1 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:23, 39.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9, loss 0.0084, train acc 0.799, test acc 0.917, time 25.8 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "938it [00:23, 39.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10, loss 0.0084, train acc 0.800, test acc 0.903, time 25.8 sec\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        " model =model.cuda()\n",
        "\n",
        "opt = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[10, 20], gamma=0.1)\n",
        "\n",
        "\n",
        "for epoch in range(0, 10):\n",
        "  n, start = 0, time.time()\n",
        "  train_l_sum = torch.tensor([0.0], dtype=torch.float32)\n",
        "  train_acc_sum = torch.tensor([0.0], dtype=torch.float32)\n",
        "  for i, (imgs, labels) in tqdm.tqdm(enumerate(mnist_trainloader)):\n",
        "    model.train()\n",
        "    # If training on GPU\n",
        "    if torch.cuda.is_available():\n",
        "      imgs = imgs.cuda()\n",
        "      labels = labels.cuda()\n",
        "      train_l_sum = train_l_sum.cuda()\n",
        "      train_acc_sum = train_acc_sum.cuda()\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # loss function\n",
        "    output =model(imgs)\n",
        "\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Calculate training error\n",
        "    model.eval()\n",
        "    labels = labels.long()\n",
        "    train_l_sum += loss.float()\n",
        "    train_acc_sum += (torch.sum((torch.argmax(output, dim=1) == labels))).float()\n",
        "    n += labels.shape[0]\n",
        "  model_scheduler.step()\n",
        "  test_acc = test_accuracy(iter(mnist_testloader),model)\n",
        "  print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' \\\n",
        "        % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time()-start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIUzWxoTKR3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuBocPPlZSpzU/lIILSRcd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}