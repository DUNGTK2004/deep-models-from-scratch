{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaEeE7gnt4wcmrs4yIcI9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DUNGTK2004/deep-models-from-scratch/blob/main/resnet_multitask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dependencies"
      ],
      "metadata": {
        "id": "JO-CL40fZhpq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0QiWEVD0ndrv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcHUbmL4ngza",
        "outputId": "3804083a-17a0-42ee-a13c-f1fa5b9a41d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "yhjd0_B4ZYfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.repeat(3, 1, 1)),])\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "vVsxLguoMX4c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customize MNIST dataset to add 1 label for datasets"
      ],
      "metadata": {
        "id": "g3uH3teq1WQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMnist(Dataset):\n",
        "  def __init__(self, root, train_bol, download, transform=None):\n",
        "    self.mnist = datasets.MNIST(root=root, train=train_bol, download=download, transform=transform)\n",
        "    self.curve = [0, 2, 3, 5, 6, 8, 9]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.mnist)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image, label = self.mnist[idx]\n",
        "    label_curve = 1 if label in self.curve else 0\n",
        "    return image, label, label_curve\n"
      ],
      "metadata": {
        "id": "zPvL9VFOsoNo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_trainset = CustomMnist(root='./data', train_bol=True, download=True, transform=transform)\n",
        "mnist_testset = CustomMnist(root='./data', train_bol=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "_541mpUtnrt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291a9175-cfb8-48eb-9909-d6473ae47d1a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.76MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "UrbEB6w5q5XM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize data"
      ],
      "metadata": {
        "id": "0Wy4F5jopsa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If error, only need change 3 -> 1 in transforms\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "  sample_idx = torch.randint(len(mnist_trainset), size=(1,)).item()\n",
        "  img, label = mnist_trainset[sample_idx]\n",
        "  figure.add_subplot(rows, cols, i)\n",
        "  plt.title(label)\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img.squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "0YZ0pOlTwFDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "net = models.resnet18(pretrained=False)\n",
        "net.fc = nn.Linear(512, 10, bias=True)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "an-CbsQM9Bba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet from scratch"
      ],
      "metadata": {
        "id": "5chmrmagZT2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self, in_channels, channels, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(channels)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    self.conv2 = nn.Conv2d(channels, channels, stride=1, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(channels)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if in_channels != channels or stride == 2:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, channels, kernel_size=1, stride=2),\n",
        "          nn.BatchNorm2d(channels)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out += self.shortcut(x)\n",
        "    out = self.relu2(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class resnetModel(nn.Module):\n",
        "  def __init__(self, basic_block, num_blocks, zero_init_residual=False):\n",
        "    super(resnetModel, self).__init__()\n",
        "    self.in_channel = 64\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    self.layer1 = self._make_layer(1, num_blocks[0], 64, basic_block)\n",
        "    self.layer2 = self._make_layer(2, num_blocks[1], 128, basic_block)\n",
        "    self.layer3 = self._make_layer(2, num_blocks[2], 256, basic_block)\n",
        "    self.layer4 = self._make_layer(2, num_blocks[3], 512, basic_block)\n",
        "    self.averagePooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(in_features=512, out_features=10)\n",
        "    self.fc2 = nn.Linear(in_features=512, out_features=2)\n",
        "\n",
        "  def _make_layer(self, stride_first, num_block, channel, basic_block):\n",
        "    strides = [stride_first] + [1] * (num_block-1)\n",
        "    layer = []\n",
        "    for i in range(num_block):\n",
        "      stride = strides[i]\n",
        "      layer.append(basic_block(self.in_channel, channel, stride))\n",
        "      self.in_channel = channel\n",
        "    return nn.Sequential(*layer)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.averagePooling(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x1 = self.fc(x)\n",
        "    x2 = self.fc2(x)\n",
        "\n",
        "    return x1, x2\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "\n",
        "model = resnetModel(BasicBlock, [2, 2, 2, 2])\n",
        "model.apply(weights_init)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRNBU-2lHtL9",
        "outputId": "62fb694b-5173-4b1d-d9f5-f2f6b3f04377"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "resnetModel(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu1): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (averagePooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")"
      ],
      "metadata": {
        "id": "RatdSUj5M8HF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1a0581-de3e-4148-fa36-b9f987127d4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 11187468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(data_iter, net):\n",
        "  acc_sum_lb, acc_sum_curve, n = 0, 0, 0\n",
        "  for (imgs, labels, label_curves) in data_iter:\n",
        "    # send data to the GPU if cuda is available\n",
        "    if torch.cuda.is_available():\n",
        "      imgs = imgs.cuda()\n",
        "      labels = labels.cuda()\n",
        "      label_curves = label_curves.cuda()\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "      labels = labels.long()\n",
        "      label_curves = label_curves.long()\n",
        "      out_lb, out_curve = net(imgs)\n",
        "      acc_sum_lb += torch.sum((torch.argmax(out_lb, dim=1) == labels)).float()\n",
        "      acc_sum_curve += torch.sum((torch.argmax(out_curve, dim=1) == label_curves)).float()\n",
        "\n",
        "      n += labels.shape[0]\n",
        "\n",
        "  return acc_sum_lb.item()/n, acc_sum_curve.item()/n"
      ],
      "metadata": {
        "id": "mxESFN8qKI2r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train / test"
      ],
      "metadata": {
        "id": "rb5KAWV3p8lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        " model =model.cuda()\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0005)\n",
        "\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(0, 10):\n",
        "  n, start = 0, time.time()\n",
        "  train_l_sum = torch.tensor([0.0], dtype=torch.float32)\n",
        "  train_acc_sum_lb = torch.tensor([0.0], dtype=torch.float32)\n",
        "  train_acc_sum_curve = torch.tensor([0.0], dtype=torch.float32)\n",
        "  for i, (imgs, labels, label_curves) in tqdm.tqdm(enumerate(mnist_trainloader)):\n",
        "    model.train()\n",
        "    # If training on GPU\n",
        "    if torch.cuda.is_available():\n",
        "      imgs = imgs.cuda()\n",
        "      labels = labels.cuda()\n",
        "      label_curves = label_curves.cuda()\n",
        "      train_l_sum = train_l_sum.cuda()\n",
        "      train_acc_sum_lb = train_acc_sum_lb.cuda()\n",
        "      train_acc_sum_curve = train_acc_sum_curve.cuda()\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # loss function\n",
        "    output_lb, output_curve =model(imgs)\n",
        "\n",
        "    loss1 = criterion1(output_lb, labels)\n",
        "    loss2 = criterion2(output_curve, label_curves)\n",
        "    loss = loss1 + loss2\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Calculate training error\n",
        "    model.eval()\n",
        "    labels = labels.long()\n",
        "    train_l_sum += loss.float()\n",
        "    train_acc_sum_lb += (torch.sum((torch.argmax(output_lb, dim=1) == labels))).float()\n",
        "    train_acc_sum_curve += (torch.sum((torch.argmax(output_curve, dim=1) == label_curves))).float()\n",
        "\n",
        "    n += labels.shape[0]\n",
        "  test_acc_lb, test_acc_curve = test_accuracy(iter(mnist_testloader), model)\n",
        "  print('epoch %d, loss %.4f, train acc lb %.3f, train_acc_curve %.3f, test acc lb %.3f, test_acc_curve %.3f, time %.1f sec' \\\n",
        "        % (epoch + 1, train_l_sum/n, train_acc_sum_lb/n, train_acc_sum_curve/n, test_acc_lb, test_acc_curve, time.time()-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2J-lpm_CkdM",
        "outputId": "99287fd1-18ab-48ff-c5e7-8d6325ffbdb6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 0.0002, train acc lb 0.994, train_acc_curve 0.998, test acc lb 0.985, test_acc_curve 0.993, time 14.7 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, loss 0.0001, train acc lb 0.997, train_acc_curve 0.999, test acc lb 0.987, test_acc_curve 0.996, time 14.8 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 36.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, loss 0.0001, train acc lb 0.998, train_acc_curve 0.999, test acc lb 0.990, test_acc_curve 0.995, time 14.7 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, loss 0.0000, train acc lb 0.999, train_acc_curve 1.000, test acc lb 0.991, test_acc_curve 0.996, time 14.8 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:12, 36.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, loss 0.0000, train acc lb 1.000, train_acc_curve 1.000, test acc lb 0.990, test_acc_curve 0.997, time 15.3 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, loss 0.0000, train acc lb 1.000, train_acc_curve 1.000, test acc lb 0.991, test_acc_curve 0.997, time 14.8 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, loss 0.0000, train acc lb 1.000, train_acc_curve 1.000, test acc lb 0.989, test_acc_curve 0.994, time 14.7 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8, loss 0.0000, train acc lb 1.000, train_acc_curve 1.000, test acc lb 0.991, test_acc_curve 0.996, time 14.9 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9, loss 0.0000, train acc lb 1.000, train_acc_curve 1.000, test acc lb 0.991, test_acc_curve 0.997, time 14.7 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "469it [00:13, 35.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10, loss 0.0000, train acc lb 1.000, train_acc_curve 1.000, test acc lb 0.992, test_acc_curve 0.996, time 15.5 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MriK8xgKl1ni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}